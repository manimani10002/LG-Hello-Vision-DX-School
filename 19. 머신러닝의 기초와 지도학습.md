## ğŸ“… 19ì¼ì°¨ TIL: ë¨¸ì‹ ëŸ¬ë‹ì˜ ê¸°ì´ˆì™€ ì§€ë„í•™ìŠµ

### 1. ë„êµ¬ì˜ ì´í•´: Numpy & Pandas

* **Numpy (ê³„ì‚°ê¸°):** íŒŒì´ì¬ì˜ ë¦¬ìŠ¤íŠ¸ë³´ë‹¤ í›¨ì”¬ ë¹ ë¥¸ **ndArray** êµ¬ì¡°ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤. ëª¨ë“  ìˆ˜ì¹˜ë¥¼ ë²¡í„°ì™€ í–‰ë ¬ë¡œ ë³€í™˜í•˜ì—¬ ì»´í“¨í„°ê°€ ì´ˆê³ ì† ì—°ì‚°ì„ í•  ìˆ˜ ìˆê²Œ ë•ìŠµë‹ˆë‹¤.
* **Pandas (ì¥ë¶€):** Numpyë¥¼ ê¸°ë°˜ìœ¼ë¡œ í•˜ì§€ë§Œ, ì‚¬ëŒì´ ë³´ê¸° í¸í•˜ë„ë¡ **í–‰(Index)ê³¼ ì—´(Column)** ì˜ ì´ë¦„ì„ ë¶™ì¸ DataFrame êµ¬ì¡°ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤. ë°ì´í„° ì „ì²˜ë¦¬ì˜ í•µì‹¬ ë„êµ¬ì…ë‹ˆë‹¤.

### 2. ë¨¸ì‹ ëŸ¬ë‹ì˜ ë³¸ì§ˆê³¼ ëª©ì 

* **ì •ì˜:** ëŒ€ëŸ‰ì˜ ë°ì´í„°ì—ì„œ í†µê³„ì  ê·œì¹™(Pattern)ì„ ì°¾ì•„ë‚´ì–´ ë¯¸ì§€ì˜ ë°ì´í„°ë¥¼ **ì˜ˆì¸¡** í•˜ëŠ” ê²ƒ.
* **ë°ì´í„° ë§ˆì´ë‹ vs ë¨¸ì‹ ëŸ¬ë‹:** * **ë°ì´í„° ë§ˆì´ë‹:** ë°ì´í„° ì†ì—ì„œ 'ëª°ëë˜ ì¸ì‚¬ì´íŠ¸'ë¥¼ ë°œê²¬í•˜ëŠ” ê²ƒì´ ì£¼ëœ ëª©ì .
* **ë¨¸ì‹ ëŸ¬ë‹:** ê³¼ê±°ì˜ ë°ì´í„°ë¥¼ í•™ìŠµí•˜ì—¬ ë¯¸ë˜ì˜ ê²°ê³¼($y$)ë¥¼ ì •í™•íˆ 'ì˜ˆì¸¡'í•˜ëŠ” ê²ƒì´ ëª©ì .


* **í‰ê°€ ì§€í‘œ:** ëª¨ë¸ì˜ ìš°ìˆ˜ì„±ì€ ì „ë¬¸ê°€ì˜ ì£¼ê´€ì´ ì•„ë‹Œ, ì˜¤ì°¨(Error)ì˜ í¬ê¸°ë¡œ ê²°ì •ë©ë‹ˆë‹¤.
* $\hat{y}$ (ëª¨ë¸ì˜ ì˜ˆì¸¡ê°’) $\approx$ $y$ (ì‹¤ì œ ì •ë‹µ) ì´ë¼ë©´ ì¢‹ì€ ëª¨ë¸!



### 3. í•™ìŠµì˜ ë¶„ë¥˜ (ì§€ë„ vs ë¹„ì§€ë„)

* **ì§€ë„ í•™ìŠµ (Supervised Learning):** ì •ë‹µ($y$)ì´ ìˆëŠ” ë°ì´í„°ë¥¼ í•™ìŠµ. (ì˜ˆ: íƒ€ì´íƒ€ë‹‰ ìƒì¡´ì ì˜ˆì¸¡)
* **ë¹„ì§€ë„ í•™ìŠµ (Unsupervised Learning):** ì •ë‹µ($y$) ì—†ì´ ë°ì´í„°ì˜ ìœ ì‚¬ì„±ë§Œìœ¼ë¡œ ê·¸ë£¹í•‘. (ì˜ˆ: ê³ ê° êµ°ì§‘í™”, ì¶”ì²œ ì‹œìŠ¤í…œì˜ ì´ˆê¸° í•„í„°ë§)
> *ì°¸ê³ : ì¶”ì²œ ì‹œìŠ¤í…œì€ ì‚¬ìš©ìì˜ ê³¼ê±° 'í´ë¦­/êµ¬ë§¤(ì •ë‹µ)'ë¥¼ í•™ìŠµí•˜ë©´ ì§€ë„í•™ìŠµì  ì„±ê²©ë„ ê°€ì§‘ë‹ˆë‹¤.*



### 4. ë¨¸ì‹ ëŸ¬ë‹ì˜ ë¶„ì„ íë¦„ (Pipeline)

1. **ëª©í‘œ ì„¤ì •:** ì¢…ì†ë³€ìˆ˜($y$, Target) ê²°ì • (ì˜ˆ: ìƒì¡´ ì—¬ë¶€).
2. **ë³€ìˆ˜ ì„ íƒ:** ë…ë¦½ë³€ìˆ˜($x$, Feature) ì¶”ì¶œ (ì˜ˆ: ì„±ë³„, ë‚˜ì´, ê°ì‹¤ ë“±ê¸‰).
3. **ë°ì´í„° ì „ì²˜ë¦¬ (ì¤‘ìš”):** ê²°ì¸¡ì¹˜(NaN) ì²˜ë¦¬, ë¬¸ìë¥¼ ìˆ«ìë¡œ ë°”ê¾¸ëŠ” ì¸ì½”ë”©, ì´ìƒì¹˜ ì œê±°.
4. **ëª¨ë¸ í•™ìŠµ:** ì•Œê³ ë¦¬ì¦˜ì„ ì„ íƒí•´ ë°ì´í„°ì˜ íŒ¨í„´ í•™ìŠµ.
5. **í‰ê°€:** ë³„ë„ì˜ í…ŒìŠ¤íŠ¸ ë°ì´í„°ë¡œ ì •í™•ë„ ì¸¡ì •.

---

## ğŸ’» ì§€ë„í•™ìŠµ ì‹¤ìŠµ ì½”ë“œ (ë¶“ê½ƒ ë°ì´í„° ë¶„ë¥˜)

í•„ê¸°í•˜ì‹  íë¦„ì— ë§ì¶°, ë°ì´í„°ë¥¼ ì½ê³  ë¶„ë¦¬í•˜ì—¬ í•™ìŠµí•˜ê³  í‰ê°€í•˜ëŠ” ì „ì²´ ê³¼ì •ì„ ì£¼ì„ê³¼ í•¨ê»˜ ì‘ì„±í–ˆìŠµë‹ˆë‹¤.

```python
from sklearn.datasets import load_iris
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
import pandas as pd

# 1. ë°ì´í„° ë¡œë”© (Numpy í˜•íƒœì˜ Bunch ê°ì²´)
iris = load_iris()
iris_data = iris.data    # ë…ë¦½ë³€ìˆ˜(x) : í”¼ì²˜
iris_label = iris.target # ì¢…ì†ë³€ìˆ˜(y) : íƒ€ê²Ÿ(ì •ë‹µ)

# 2. ë¶„ì„ì„ ìœ„í•´ Pandas DataFrameìœ¼ë¡œ ë³€í™˜
iris_df = pd.DataFrame(data=iris_data, columns=iris.feature_names)
iris_df['label'] = iris.target # ì •ë‹µ ì»¬ëŸ¼ ì¶”ê°€

# 3. ë°ì´í„° ì „ì²˜ë¦¬ ë‹¨ê³„ (ì˜ˆì‹œ: ê²°ì¸¡ì¹˜ í™•ì¸)
# ì‹¤ì œ ë°ì´í„°ì…‹(íƒ€ì´íƒ€ë‹‰ ë“±)ì—ì„œëŠ” ì—¬ê¸°ì„œ fillna() ë“±ì„ ìˆ˜í–‰í•¨
print(iris_df.isnull().sum()) 

# 4. ë°ì´í„° ë¶„í•  (ê³µë¶€ìš© ë¬¸ì œì§‘/ì •ë‹µì§€ vs ì‹œí—˜ìš© ë¬¸ì œì§‘/ì •ë‹µì§€)
# test_size=0.2 : 20%ëŠ” ì‹œí—˜ìš©ìœ¼ë¡œ ë‚¨ê²¨ë‘ 
X_train, X_test, y_train, y_test = train_test_split(
    iris_data, iris_label, test_size=0.2, random_state=11
)

# 5. ì•Œê³ ë¦¬ì¦˜ ì„ íƒ ë° ëª¨ë¸ ê°ì²´ ìƒì„± (ì˜ì‚¬ê²°ì •ë‚˜ë¬´)
dt_clf = DecisionTreeClassifier(random_state=11)

# 6. ëª¨ë¸ í•™ìŠµ (ì§€ë„í•™ìŠµ: ë¬¸ì œì™€ ì •ë‹µì„ í•¨ê»˜ ë˜ì ¸ì¤Œ)
dt_clf.fit(X_train, y_train)

# 7. ì˜ˆì¸¡ ìˆ˜í–‰ (ì‹œí—˜ ë¬¸ì œì§€ë§Œ ì£¼ê³  AIì—ê²Œ í’€ì–´ë³´ë¼ê³  í•¨)
pred = dt_clf.predict(X_test)

# 8. í‰ê°€ (AIê°€ í‘¼ ë‹µ(pred)ê³¼ ì‹¤ì œ ì •ë‹µ(y_test)ì„ ë¹„êµ)
print(f'ì˜ˆì¸¡ ì •í™•ë„: {accuracy_score(y_test, pred):.4f}')

```
