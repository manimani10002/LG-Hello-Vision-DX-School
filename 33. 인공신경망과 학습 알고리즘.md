# 📘 33일차 TIL: 인공신경망과 학습 알고리즘

## 1. 인공신경망의 최소 단위: 퍼셉트론 (Perceptron)

인공신경망은 인간의 뇌세포(뉴런)를 모사한 **노드(Node)**들로 이루어져 있습니다.

* **입력(Input):** 수집한 데이터 (예: VOD 시청 시간, 연령대 등)
* **가중치(Weight, $w$):** 각 입력이 정답을 맞히는 데 얼마나 중요한지 나타내는 '중요도'
* **편향(Bias, $b$):** 노드가 얼마나 쉽게 활성화될지 조절하는 '민감도'
* **계산:** 입력값에 가중치를 곱하고 편향을 더해 하나의 숫자를 만듭니다. ($y = wx + b$)

---

## 2. '선형'의 한계와 '활성화 함수'

하나의 노드는 수학적으로 **직선(선형)**만 그을 수 있습니다.

* **단층 퍼셉트론의 한계:** 노드가 아무리 많아도 층이 하나뿐이면, 그 결과들을 합쳤을 때 결국 **'거대한 직선 하나'**가 되어버립니다. 그래서 XOR 문제처럼 직선 하나로 나눌 수 없는 복잡한 데이터는 해결하지 못합니다.
* **활성화 함수 (Activation Function):** 이 한계를 깨기 위해 노드 끝에 붙이는 '가위'입니다.
* 계산된 결과(직선)를 **특정 지점에서 꺾거나 비틉니다.** (예: ReLU는 음수를 0으로 잘라버림)
* 이 과정을 통해 **비선형성(Non-linearity)**이 생기며, 비로소 곡선 형태의 판단 기준을 만들 준비가 됩니다.



---

## 3. 층(Layer)의 구조: 너비 vs 깊이

신경망은 크게 세 가지 층으로 구성됩니다.

| 구성 요소 | 역할 | 비유 |
| --- | --- | --- |
| **입력층 (Input Layer)** | 데이터를 받아들임 | 오감 (시각, 청각 등) |
| **은닉층 (Hidden Layer)** | 데이터 속 특징을 추출하고 조합함 | 뇌의 사고 과정 |
| **출력층 (Output Layer)** | 최종 예측 결과를 내놓음 | 최종 판단 (예/아니오) |

* **너비 (Width, 노드 수):** 한 층에 노드가 많으면 데이터를 **다양한 각도의 직선**으로 세밀하게 관찰할 수 있습니다. (재료가 풍부해짐)
* **깊이 (Depth, 층 수):** 층이 여러 개면 앞 층이 만든 직선들을 **조합하고 추상화**하여 매우 복잡한 패턴(곡선)을 학습할 수 있습니다. (요리가 정교해짐)

---

## 4. 학습의 핵심: 하이퍼파라미터 튜닝

잘 만든 인공신경망이란 **'적절한 너비와 깊이'**를 가진 모델을 말합니다.

* **과소적합 (Underfitting):** 층과 노드가 너무 적어 데이터의 패턴을 전혀 못 읽는 상태.
* **과적합 (Overfitting):** 층과 노드가 너무 많아 데이터의 잡음까지 다 외워버린 상태. (암기 위주 공부)
* **엔지니어의 역할:** 실험을 통해 우리 데이터(VOD 시청 이력)에 딱 맞는 최적의 층 수와 노드 수(**하이퍼파라미터**)를 찾아내는 것이 핵심입니다.

---

## 5. 역전파 알고리즘

인공신경망의 핵심 엔진인 **순전파(Forward Propagation)**와 **역전파(Backpropagation)**를 정리합니다.

### 1) 학습의 전체 흐름: [예측 → 채점 → 수정]

* **순전파:** 문제를 푸는 과정 (**예측**)
* **손실 함수:** 정답과 비교해 점수를 매기는 과정 (**채점**)
* **역전파:** 틀린 이유를 찾아 가중치를 고치는 과정 (**수정/오답노트**)

### 2) 순전파 알고리즘 (Forward Propagation)

데이터가 **입력층 → 은닉층 → 출력층** 방향으로 흐르며 최종 예측값을 계산합니다.

1. **입력:** 데이터($x$)가 들어오면 각 노드에 설정된 **가중치($w$)**를 곱하고 **편향($b$)**을 더합니다.
2. **활성화:** 합계 금액을 **활성화 함수**에 통과시켜 비선형 특징을 추출합니다.
3. **전달:** 꺾인 신호들이 다음 층의 노드들로 전달되어 다시 조합됩니다.
4. **출력:** 최종 결과값을 내놓습니다.

### 3) 손실 함수 (Loss Function): "채점 단계"

예측값과 실제 정답(Label)을 비교하여 오차를 계산합니다. 오차가 크면 역전파를 통해 가중치를 대폭 수정하라는 신호가 됩니다.

### 4) 역전파 알고리즘 (Backpropagation)

출력층에서 발생한 오차를 거꾸로 전달하며 **가중치($w$)와 편향($b$)을 수정**합니다.

1. **미분(Gradient) 계산:** 최종 오차에 대해 각 노드의 가중치가 얼마나 기여했는지 계산합니다.
2. **연쇄 법칙(Chain Rule):** 오차를 아주 작은 조각으로 나누어 이전 층으로 배분합니다.
3. **가중치 업데이트:** 계산된 기여도만큼 가중치를 조절합니다.

---

## 6. 활성화 함수 (Activation Function) 상세

### 1) 주요 종류와 특징

| 종류 | 특징 | 장단점 및 비유 |
| --- | --- | --- |
| **Sigmoid** | 0과 1 사이의 S자 곡선 | 확률 표현에 유리하나, 깊은 층에서 **기울기 소실** 발생 |
| **ReLU** | 0보다 작으면 0, 크면 값 그대로 | 현대 딥러닝 표준. 학습이 빠르고 기울기 소실 없음 |
| **Softmax** | 출력값의 합을 1로 만듦 | 다중 분류 문제의 **출력층**에서 확률을 보여줌 |

### 2) 결정적 이슈: 기울기 소실 (Vanishing Gradient)

* **원인:** 시그모이드는 양 끝에서 미분값이 0에 가까워집니다.
* **현상:** 역전파 시 0에 가까운 값을 계속 곱하게 되어 앞쪽 층까지 오차 신호가 전달되지 않아 학습이 중단됩니다.
* **해결:** 은닉층에서는 미분값이 유지되는 **ReLU**를 사용하여 해결합니다.

### 3) 층별 설정 기준 (Best Practice)

* **은닉층 (Hidden Layers):** 학습 효율을 위해 **ReLU** (혹은 Leaky ReLU) 추천.
* **출력층 (Output Layer):** 답안지 양식에 따라 결정.
* 이진 분류: **Sigmoid**
* 다중 분류: **Softmax**
* 수치 예측: **Linear** (함수 없음)



---

## 7. 결론: 왜 'Deep' Learning인가?

수학적으로는 은닉층이 하나만 있어도 모든 함수를 흉내 낼 수 있지만, 현실에서는 **층을 깊게 쌓는 것(Deep)**이 훨씬 적은 노드로도 효율적이고 정확하게 데이터를 이해하기 때문입니다.

---

**정리된 TIL 내용 중 추가하고 싶거나 더 자세한 설명이 필요한 부분이 있으면 언제든 말씀해 주세요!**
