# ğŸ“… 22ì¼ì°¨ TIL: ë¨¸ì‹ ëŸ¬ë‹ ë°ì´í„° ì „ì²˜ë¦¬ & ì•Œê³ ë¦¬ì¦˜(ì˜ì‚¬ê²°ì •ë‚˜ë¬´)

## 1. ë°ì´í„° ì „ì²˜ë¦¬: ìˆ˜ì¹˜í™”ì™€ í˜•í‰ì„± (Scaling & Encoding)

ë¨¸ì‹ ëŸ¬ë‹ ëª¨ë¸ì€ ìˆ˜í•™ì  ì—°ì‚°ì„ ìˆ˜í–‰í•˜ë¯€ë¡œ, ë°ì´í„°ì˜ **'í˜•íƒœ'** ì™€ **'ì²´ê¸‰'** ì„ ë§ì¶”ëŠ” ê²ƒì´ ì²« ë²ˆì§¸ ë‹¨ê³„ì…ë‹ˆë‹¤.

### â‘  í”¼ì²˜ ìŠ¤ì¼€ì¼ë§ (Feature Scaling): "ì—´(Column) ê°„ì˜ ì²´ê¸‰ ë§ì¶”ê¸°"

* **í‘œì¤€í™” (Standardization):** í‰ê·  0, ë¶„ì‚° 1ë¡œ ë³€í™˜. ì´ìƒì¹˜ê°€ ìˆì–´ë„ ë¹„êµì  ì•ˆì „í•˜ë©°, ë°ì´í„°ê°€ ì •ê·œë¶„í¬ë¥¼ ë”°ë¥¼ ë•Œ ìœ ë¦¬í•¨.
* **Min-Max ìŠ¤ì¼€ì¼ë§:** ëª¨ë“  ê°’ì„ 0~1 ì‚¬ì´ë¡œ ì••ì¶•. ë”¥ëŸ¬ë‹ì—ì„œ ì„ í˜¸ë˜ë‚˜ ì´ìƒì¹˜(Outlier)ì— ë§¤ìš° ì·¨ì•½í•¨.
* **í•µì‹¬:** ìŠ¤ì¼€ì¼ë§ì„ í•´ì•¼ ê²½ì‚¬í•˜ê°•ë²• ì‹œ 'ìš¸í‰ë¶ˆí‰'í•˜ê²Œ ê¸¸ì„ í—¤ë§¤ì§€ ì•Šê³  ìµœì ì˜ ê°€ì¤‘ì¹˜($w$)ë¥¼ ë¹ ë¥´ê²Œ ì°¾ìŒ.

### â‘¡ ì¸ì½”ë”© (Encoding): "ë¬¸ìë¥¼ ìˆ«ìë¡œ ë°”ê¾¸ê¸°"

* **ë ˆì´ë¸” ì¸ì½”ë”©:** 'ì‚¬ê³¼:1, ë°°:2'ì‹ìœ¼ë¡œ ë²ˆí˜¸ ë¶€ì—¬. íŠ¸ë¦¬ ê¸°ë°˜ ëª¨ë¸ì— ì í•©í•˜ë‚˜, ìˆ«ìì˜ í¬ê¸°ê°€ ëª¨ë¸ì— ì˜¤í•´ë¥¼ ì¤„ ìˆ˜ ìˆìŒ.
* **ì›-í•« ì¸ì½”ë”©:** ê° í•­ëª©ì„ ë…ë¦½ëœ ì»¬ëŸ¼(1ê³¼ 0)ìœ¼ë¡œ ë¶„ë¦¬. í•­ëª© ê°„ ì„œì—´ì´ ì—†ì„ ë•Œ í•„ìˆ˜. ê²°ê³¼ëŠ” ëŒ€ë¶€ë¶„ 0ì¸ **í¬ì†Œ í–‰ë ¬(Sparse Matrix)** í˜•íƒœê°€ ë¨.

---

## 2. ìƒ˜í”Œë§: ë°ì´í„°ì˜ ê· í˜•ê³¼ ì¼ë°˜í™” (Sampling)

íŠ¹ì • í´ë˜ìŠ¤(ì˜ˆ: ì·¨ì†Œ $Y$ )ê°€ ë„ˆë¬´ ì ìœ¼ë©´ ëª¨ë¸ì€ ë‹¤ìˆ˜íŒŒì˜ ì˜ê²¬ë§Œ ë”°ë¥´ëŠ” í¸í–¥ëœ ëª¨ë¸ì´ ë©ë‹ˆë‹¤.

* **ë‹¤ìš´ìƒ˜í”Œë§:** ë‹¤ìˆ˜ ë°ì´í„°ë¥¼ ì¤„ì„. ì†ë„ëŠ” ë¹ ë¥´ë‚˜ ì •ë³´ ì†ì‹¤ ìœ„í—˜ì´ ìˆìŒ.
* **ì—…ìƒ˜í”Œë§:** ì†Œìˆ˜ ë°ì´í„°ë¥¼ ëŠ˜ë¦¼. ì •ë³´ ì†ì‹¤ì€ ì—†ìœ¼ë‚˜ **ê³¼ì í•©(Overfitting)** ìœ„í—˜ì´ ìˆìŒ.
* **SMOTE ê¸°ë²•:** ë‹¨ìˆœ ë³µì‚¬ê°€ ì•„ë‹Œ, ì†Œìˆ˜ ë°ì´í„° ì‚¬ì´ì˜ ê°’ì„ ë³´ê°„í•˜ì—¬ ê°€ì§œ ë°ì´í„°ë¥¼ ìƒì„±í•˜ëŠ” íš¨ìœ¨ì ì¸ ë°©ë²•.



---

## 3. ë¨¸ì‹ ëŸ¬ë‹ì˜ ì² í•™: ì¼ë°˜í™”(Generalization)

* **ëª©ì :** ê³¼ê±° ë°ì´í„°ì—ë§Œ ì™„ë²½í•œ ëª¨ë¸ì´ ì•„ë‹ˆë¼, **ë¯¸ë˜ì˜ ìƒˆë¡œìš´ ë°ì´í„°** ë¥¼ ì˜ ë§ì¶”ëŠ” ëª¨ë¸ì„ ë§Œë“œëŠ” ê²ƒ.
* **ì´ìƒì¹˜ ì²˜ë¦¬:** í†µê³„í•™ì²˜ëŸ¼ ë¬´ì¡°ê±´ ì œê±°í•˜ê¸°ë³´ë‹¤, í˜„ì‹¤ì—ì„œ ë°œìƒ ê°€ëŠ¥í•œ ë²”ìœ„ë¼ë©´ ëª¨ë¸ì´ í•™ìŠµí•˜ê²Œ í•˜ì—¬ 'ë³´í¸ì„±'ì„ ê°–ì¶”ê²Œ í•¨.
* **ì£¼ì˜:** í•™ìŠµ ë°ì´í„°ì— ê³¼í•˜ê²Œ ì§‘ì°©í•˜ëŠ” **ê³¼ì í•©** ì„ ê²½ê³„í•´ì•¼ í•¨.

---

## 4. ì§€ë„í•™ìŠµ: ì˜ì‚¬ê²°ì •ë‚˜ë¬´ (Decision Tree)

ë°ì´í„° ì† ê·œì¹™ì„ ì°¾ì•„ 'ìŠ¤ë¬´ê³ ê°œ'ì²˜ëŸ¼ ë¶„ë¥˜í•´ ë‚˜ê°€ëŠ” ì§ê´€ì ì¸ ì•Œê³ ë¦¬ì¦˜ì…ë‹ˆë‹¤.

### â‘  ìµœì  ë¶„í•  ì›ë¦¬: "ë¶ˆìˆœë„ ê°ì†ŒëŸ‰ ìµœëŒ€í™”"

* **ë¶ˆìˆœë„(Impurity):** ë°ì´í„°ê°€ ì„ì—¬ ìˆëŠ” ì •ë„.
* **í•µì‹¬:** í•œ ë²ˆ ê°€ì§€ë¥¼ ì¹  ë•Œë§ˆë‹¤ ì„ì—¬ ìˆë˜ ë°ì´í„°ë“¤ì´ ê°€ì¥ ê¹”ë”í•˜ê²Œ(ìˆœìˆ˜í•˜ê²Œ) ë‚˜ë‰˜ëŠ” ê¸°ì¤€ì„ ì°¾ìŒ. ì´ë•Œ ë°œìƒí•˜ëŠ” **'ì •ë³´ ì´ë“'** ì´ ìµœëŒ€ê°€ ë˜ëŠ” ì§€ì ì„ ì„ íƒ.

### â‘¡ ê³¼ì í•© í•´ê²° (ê°€ì§€ì¹˜ê¸°)

ë‚˜ë¬´ê°€ ë„ˆë¬´ ê¹Šì–´ì§€ë©´ í•™ìŠµ ë°ì´í„°ì—ë§Œ íŠ¹í™”ë˜ë¯€ë¡œ ì„±ì¥ì„ ì œí•œí•´ì•¼ í•¨.

* `max_depth`: íŠ¸ë¦¬ì˜ ìµœëŒ€ ê¹Šì´ ì œí•œ.
* `min_samples_split`: ë¶„í• í•˜ê¸° ìœ„í•œ ìµœì†Œ ìƒ˜í”Œ ìˆ˜ ì§€ì •.

---

## 5. Python í†µí•© ì‹¤ì „ ì½”ë“œ ì˜ˆì‹œ

```python
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.tree import DecisionTreeClassifier
from imblearn.over_sampling import SMOTE

# 1. ë°ì´í„° ë¡œë“œ (ê°€ì •)
# df = pd.read_csv('data.csv')

# 2. ì¸ì½”ë”© (ë¬¸ì -> ìˆ«ì)
df_encoded = pd.get_dummies(df, columns=['category_column'])

# 3. ë°ì´í„° ë¶„ë¦¬
X = df_encoded.drop('target', axis=1)
y = df_encoded['target']
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# 4. ìƒ˜í”Œë§ (ë¶ˆê· í˜• í•´ì†Œ)
smote = SMOTE(random_state=42)
X_train_over, y_train_over = smote.fit_resample(X_train, y_train)

# 5. ìŠ¤ì¼€ì¼ë§ (ì²´ê¸‰ ë§ì¶”ê¸°)
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train_over)
X_test_scaled = scaler.transform(X_test) # TestëŠ” ë°˜ë“œì‹œ transformë§Œ!

# 6. ëª¨ë¸ í•™ìŠµ (ì˜ì‚¬ê²°ì •ë‚˜ë¬´)
model = DecisionTreeClassifier(max_depth=5, random_state=42)
model.fit(X_train_scaled, y_train_over)

# 7. í‰ê°€
accuracy = model.score(X_test_scaled, y_test)
print(f"ìµœì¢… ëª¨ë¸ ì •í™•ë„: {accuracy:.4f}")

```
