# 📘 31일차 TIL: 텍스트 분석 및 Word2Vec 심화 학습

### 1. 텍스트 분석의 흐름: 왜 숫자로 바꾸는가?

컴퓨터는 텍스트(문자)를 직접 이해할 수 없습니다. 따라서 모든 텍스트 데이터는 계산 가능한 **수치 데이터(Vector)**로 변환되어야 합니다.

* **전처리 과정:** 문서 → 문장 → 단어 → 형태소(Tokenizing).
* *추가:* 단순히 자르는 것뿐만 아니라, 의미 없는 단어(불용어, Stopwords) 제거와 어근 추출(Lemmatization/Stemming) 과정이 포함됩니다.


* **벡터화(Vectorization):** 텍스트를 고정된 길이의 벡터(숫자 리스트)로 바꾸는 과정을 의미합니다.

---

### 2. 단어 표현 방식의 진화 (BoW vs Word2Vec)

| 구분 | BoW (Bag of Words) | Word2Vec (Embedding) |
| --- | --- | --- |
| **특징** | 단어의 출현 빈도에 집중 | 단어 사이의 문맥적 의미에 집중 |
| **차원** | 단어 사전 크기만큼 커짐 (고차원) | 사용자가 설정한 고정 크기 (저차원) |
| **밀도** | 대부분 0인 희소 행렬 (Sparse Matrix) | 값이 꽉 찬 밀집 벡터 (Dense Vector) |
| **한계** | 단어의 의미나 순서를 무시함 | 학습 데이터가 적으면 성능이 저하됨 |

---

### 3. Word2Vec의 핵심 원리

Word2Vec은 **"비슷한 문맥에서 등장하는 단어들은 비슷한 의미를 갖는다"**는 분포 가설을 기반으로 합니다.

* **공간적 의미:** 단어들을 $N$차원의 좌표 공간에 배치합니다. 의미가 비슷할수록 두 단어 벡터 사이의 거리가 가깝고, 코사인 유사도가 높게 측정됩니다.
* **벡터 연산:** Word2Vec의 가장 놀라운 점은 단어 간 연산이 가능하다는 것입니다.
* 예: `왕` - `남자` + `여자` = `여왕`



---

### 4. Word2Vec의 두 가지 학습 방법

Word2Vec은 주변 단어(Context)와 중심 단어(Target)의 관계를 이용해 학습합니다.

#### ① CBOW (Continuous Bag of Words)

* **개념:** 주변에 있는 단어들을 입력으로 주어, **중심에 있는 단어를 예측**하는 방법입니다.
* **특징:** 여러 단어를 조합해 하나를 맞추므로 학습 속도가 빠릅니다. 데이터셋이 작을 때 유리할 수 있습니다.
* **예시:** "나는 아침에 [ ? ] 마신다" → '커피'를 예측.

#### ② Skip-gram

* **개념:** 한 개의 **중심 단어를 입력으로 주어, 주변 단어들을 예측**하는 방법입니다.
* **특징:** CBOW보다 더 어려운 문제를 푸는 셈이지만, 단어 간의 미세한 관계를 더 잘 잡아냅니다. 일반적으로 CBOW보다 성능이 좋아 더 많이 쓰입니다.
* **예시:** "[ ? ] [ ? ] 커피를 [ ? ]" → '나는', '아침에', '마신다'를 예측.

---

### 5. 벡터라이징(Vectorizing)의 진짜 의미

단순히 숫자로 바꾸는 것을 넘어, **"단어의 특징(Feature)을 추출하여 저차원의 연속적인 공간에 투영하는 것"**을 의미합니다. 이를 통해 컴퓨터는 단순한 '기호'로서의 단어가 아니라, 단어 사이의 '관계'와 '깊이'를 이해하게 됩니다.
