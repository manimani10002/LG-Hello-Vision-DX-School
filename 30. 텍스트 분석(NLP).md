# 📘 30일차 TIL: 텍스트 분석(NLP) 파이프라인: 원문에서 숫자 행렬까지

비정형 데이터인 텍스트를 머신러닝 모델이 이해할 수 있는 수치형 데이터로 변환하는 5단계 과정을 정리했습니다.

## 1단계: Text Tokenization (문장을 토큰으로)

텍스트 분석의 첫 단추로, 긴 문자열을 분석의 최소 단위인 **토큰(Token)**으로 쪼개는 과정입니다.

* **문장 토큰화:** 마침표(`.`), 물음표(`?`), 줄바꿈 등을 기준으로 문장을 분리합니다.
* **단어 토큰화:** 공백이나 구두점을 기준으로 단어를 분리합니다.
* **핵심:** 영어는 공백 기준의 토큰화가 잘 작동하지만, 한국어는 조사가 붙는 특성 때문에 단순 공백 분리보다는 의미를 가진 최소 단위인 '형태소' 단위의 토큰화가 필수적입니다.

## 2단계: 정제 - Stopwords 제거 (의미 없는 단어 걸러내기)

모든 단어가 분석에 중요한 것은 아닙니다. 분석 목적에 기여도가 낮은 단어를 **불용어(Stopwords)**라고 하며 이를 제거합니다.

* **특징:** `I`, `my`, `the`, `at` 처럼 빈도는 높지만 실제 문맥적 의미를 파악하는 데 방해가 되는 노이즈를 줄여 모델의 정확도를 높입니다.

## 3단계: 정제 - Stemming & Lemmatization (단어 원형 복원)

단어의 다양한 변형(과거형, 진행형, 복수형 등)을 하나의 원형으로 통일하여 단어 사전의 크기를 줄입니다.

* **Stemming (어근 추출):** 정해진 규칙에 따라 단어의 끝부분을 기계적으로 잘라냅니다. (예: `working`, `works` → `work`) 빠르지만 결과물이 사전에 없는 단어일 수 있습니다.
* **Lemmatization (표제어 추출):** 품사와 문법적 의미를 고려하여 단어의 근본적인 뿌리 형태를 찾습니다. (예: `am`, `is`, `are` → `be`) Stemming보다 정교합니다.

## 4단계: 벡터화 - Bag of Words (단어의 숫자로 변환)

텍스트 전처리가 끝나면 단어의 순서는 무시하고 오직 **출현 빈도(Frequency)**에만 집중하여 수치화하는 **BOW(Bag of Words)** 모델을 적용합니다.

* **Count Vectorization:** 단어가 나타난 횟수만큼 숫자를 부여합니다.
* **TF-IDF:** 단순히 자주 나오는 단어보다, 특정 문서에서만 특별하게 자주 나오는 '중요 단어'에 가중치를 주어 더 정교한 분석을 가능하게 합니다.

## 5단계: 희소행렬 압축 - COO & CSR (메모리 최적화)

텍스트 데이터는 단어 사전이 매우 크기 때문에 대부분의 값이 0인 **희소 행렬(Sparse Matrix)**이 생성됩니다. 이를 효율적으로 저장하기 위한 기법입니다.

* **COO (Coordinate):** 0이 아닌 데이터의 **값**과 해당 **행(Row)/열(Column)의 위치**를 각각 저장하는 직관적인 방식입니다.
* **CSR (Compressed Sparse Row):** COO에서 행의 위치 정보를 압축하여 메모리 효율과 연산 속도를 극대화한 방식입니다. 대규모 텍스트 분석에서 표준적으로 사용됩니다.
