# 📘 25일차 TIL: 머신러닝 심화 개념 통합 노트

## 1. 데이터 전처리 (Preprocessing): 모델의 성패를 결정하는 80%

하이퍼 파라미터 튜닝이 '자동차의 시트를 조절하는 것'이라면, 데이터 전처리는 '엔진을 정비하는 것'과 같습니다.

* **Feature Scaling:** 데이터의 단위(예: 몸무게 kg vs 키 cm)가 다르면 모델이 특정 변수에만 치우쳐 학습합니다. 이를 일정 범위로 맞추는 작업이 필수입니다.
* **이상치/결측치 처리:** 쓰레기 데이터가 들어가면 결국 오답을 내놓습니다. 전처리를 통해 데이터의 노이즈를 제거하는 것이 튜닝보다 훨씬 큰 정확도 향상을 가져옵니다.

---

## 2. 로지스틱 회귀 (Logistic Regression): 분류의 기초이자 핵심

로지스틱 회귀는 선형 회귀의 아이디어를 빌려와 **"사건이 발생할 확률"** 을 예측합니다.

### ① 왜 로직(Logit)이 필요한가?

선형 회귀식($y = wx + b$)은 결과값($y$)이 무한대까지 뻗어나갑니다. 하지만 우리는 **0~1 사이의 확률** 을 원합니다.

1. **Odds(승산):** 성공 확률을 실패 확률로 나눕니다 ($\frac{p}{1-p}$). 결과는 $0 \sim \infty$가 됩니다.
2. **Logit:** 여기에 로그를 취합니다 ($log(\frac{p}{1-p})$). 결과는 $-\infty \sim \infty$가 됩니다.
3. **결합:** 이제 이 값을 선형 회귀식($wx + b$)과 같다고 놓고 학습합니다. 즉, 직선의 방정식을 통해 확률의 로그값을 예측하는 것입니다.

### ② 시그모이드(Sigmoid) 함수

학습이 끝나면 우리는 다시 확률을 얻어야 합니다. 로짓 식을 거꾸로 계산해서 정리하면 **시그모이드 함수** 가 나옵니다. 이 함수는 어떤 값이 들어와도 S자 곡선을 그리며 0과 1 사이로 변환해줍니다.

---

## 3. XGBoost: 최강의 앙상블 알고리즘

XGBoost는 여러 개의 Decision Tree(의사결정 나무)를 결합하여 만드는 강력한 모델입니다.

### ① 작동 원리: 경사하강법 기반 부스팅

* **부스팅(Boosting):** 처음에 만든 나무가 틀린 부분(오차)을 다음 나무가 집중적으로 학습합니다. 이 과정을 반복하며 오차를 줄여나갑니다.
* **Gradient Descent(경사하강법):** 손실 함수를 미분하여 기울기를 구하고, 오차가 최소가 되는 방향으로 가중치를 업데이트합니다.

### ② 과적합 방지 시스템 (규제)

XGBoost가 다른 모델보다 뛰어난 이유는 자체적으로 **L1, L2 규제** 기능을 내장하고 있기 때문입니다.

* **`reg_alpha` (L1):** 가중치가 작은 피처들을 과감히 **0** 으로 만들어 피처를 골라냅니다.
* **`reg_lambda` (L2):** 가중치들이 너무 커지지 않게 전반적으로 눌러주어 모델의 변동성을 줄입니다.

---

## 4. 손실 함수: LogLoss (로그 손실)

분류 모델이 "얼마나 틀렸는지"를 계산하는 척도입니다.

* 정답이 1인데 모델이 0.1이라고 예측하면, 로그 함수를 통해 아주 큰 페널티를 부여합니다.
* 단순히 "맞았다/틀렸다"가 아니라 **"정답을 얼마나 확신했는가"** 를 평가하기 때문에 모델을 훨씬 정교하게 만듭니다.

---

## 5. 최적화 도구: GridSearchCV vs HyperOpt

하이퍼 파라미터는 모델이 학습하는 게 아니라 **사람이 직접 설정**해줘야 하는 값입니다.

* **GridSearchCV:** 우리가 정해준 후보(예: `max_depth` 3, 5, 7)를 **모든 조합**으로 다 해봅니다. 데이터가 많으면 하루 종일 걸릴 수도 있습니다.
* **HyperOpt (베이지안 최적화):** 똑똑한 탐색입니다. 이전 테스트 결과를 보고 "아, `max_depth`가 5일 때 점수가 좋았으니 그 근처를 더 파보자"라고 판단합니다. **최소한의 시도로 최적의 값** 을 찾아냅니다.

---

## 6. 평가 데이터의 분리 (Train/Val/Test)

* **Train:** 모델이 공부하는 책입니다.
* **Validation:** 공부가 잘되고 있는지 확인하는 **중간고사** 입니다. 이 데이터를 보고 HyperOpt가 파라미터를 수정합니다.
* **Test:** 단 한 번도 보지 못한 **최종 수능** 입니다. 이 점수가 모델의 진짜 실력입니다. 모델에 Train 데이터를 다시 넣어서 점수를 확인하는 건 "문제 답을 다 외운 상태에서 시험 보는 것"과 같아 의미가 없습니다.
