# 📅 [23일차] TIL: 의사결정나무, 모델 평가, 앙상블(RandomForest)

## 1. 의사결정나무의 핵심 개념

* **개념** : 데이터 속에 숨겨진 규칙을 '스무고개'처럼 찾아내어 나무(Tree) 구조로 분류하거나 예측하는 지도학습 알고리즘입니다.
* **작동 원리** : 데이터를 가장 잘 나눌 수 있는 질문을 순서대로 던져서 불순도를 낮추는 방향으로 학습합니다.
* **주요 용어** :
* **지니 계수 (Gini Index)** : 노드의 **불순도** 를 나타내는 지표(0~0.5)입니다. 0에 가까울수록 한 종류의 데이터만 모여있는 순수한 상태입니다.
* **불순도 (Impurity)** : 해당 범주 안에 서로 다른 데이터가 얼마나 섞여 있는지를 뜻합니다.
* **정보 이득 (Information Gain)** : 질문을 통해 불순도가 감소한 양입니다. 모델은 정보 이득을 **최대화** 하는 방향으로 기준값을 정합니다.



---

## 2. 과적합(Overfitting)과 일반화(Generalization)

* **문제 발생** : 의사결정나무는 제약이 없으면 지니 계수가 0이 될 때까지(완벽하게 나뉠 때까지) 가지를 칩니다. 이 경우 **학습 데이터에만 너무 예민하게 반응** 하여 복잡한 나무가 만들어집니다.
* **과적합** : 학습 데이터는 100% 맞히지만, 새로운 데이터(Test)는 잘 맞히지 못하는 상태입니다.
* **일반화** : 새로운 데이터에서도 안정적인 성능을 내도록 모델을 단순화하는 과정입니다.

---

## 3. 하이퍼파라미터 튜닝 (일반화 방법)

모델의 '깊이'와 '너비'를 제한하여 과적합을 방지합니다.

| 하이퍼파라미터 | 의미 | 효과 |
| --- | --- | --- |
| **max_depth** | 나무가 아래로 내려갈 수 있는 **최대 깊이** 를 제한합니다. | 너무 깊어지지 않게 하여 규칙을 단순화합니다. |
| **min_samples_split** | 노드를 분할하기 위해 필요한 **최소 샘플 수** 입니다. | 샘플이 너무 적으면 더 이상 질문을 던지지 못하게 막습니다. |
| **min_samples_leaf** | 말단 노드(Leaf)가 되기 위해 필요한 **최소 샘플 수** 입니다. | 아주 소수의 예외 데이터 때문에 가지가 생기는 것을 방지합니다. |

---

## 4. 모델 평가의 올바른 이해

* **학습률 vs 정확도** :
* **Train 데이터 평가** : 모델이 공부한 문제로 시험을 보는 것과 같습니다. 여기서 100%가 나온다면 실력이 좋은 게 아니라 문제를 '통째로 암기'한 **과적합**  상태일 확률이 높습니다.
* **Test 데이터 평가** : 한 번도 보지 못한 문제로 실력을 검증하는 **실제 정확도** 입니다.


* **평가의 진정한 의미** : 머신러닝의 목적은 공부한 데이터를 맞히는 것이 아니라, **앞으로 들어올 새로운 데이터를 맞히는 것(일반화)** 에 있음을 명심해야 합니다.

---

## 5. 실습 요약 (붓꽃 데이터 & Graphviz)

* **Graphviz 시각화** : `export_graphviz`를 통해 모델이 어떤 변수(Feature)와 기준값으로 데이터를 나누었는지 시각적으로 확인했습니다.
* **데이터 흐름** : `load_iris()` → `train_test_split()` → `DecisionTreeClassifier()` → `fit()` → `predict()` 순으로 진행되었습니다.

## 6. 앙상블 학습과 랜덤 포레스트

### 6-1. 앙상블 학습(Ensemble Learning)이란?

* **개념** : 하나의 강력한 모델에 의존하는 대신, 여러 개의 약한 모델(Weak Learner)을 결합하여 더 정확하고 강력한 예측 모형을 만드는 기법입니다.
* **핵심 원리** : "집단지성"과 비슷합니다. 여러 명의 전문가가 투표하여 결론을 내리는 것이 한 명의 판단보다 오류를 줄일 수 있다는 원리입니다.
* **주요 유형** :
1. **보팅(Voting)** : 서로 다른 알고리즘을 가진 모델들이 투표를 통해 최종 결과를 결정합니다.
2. **배깅(Bagging)** : 같은 알고리즘을 사용하되, 데이터를 샘플링하여 각각 학습시킨 후 결과를 집계합니다. (예: 랜덤 포레스트)
3. **부스팅(Boosting)** : 여러 모델을 순차적으로 학습시키며, 앞선 모델이 틀린 데이터에 가중치를 주어 학습합니다. (예: XGBoost, LightGBM)



---

### 6-2. 보팅(Voting)의 방식: Hard vs Soft

* **Hard Voting** : 다수결 원칙입니다. 각 모델이 예측한 결과값 중 가장 많이 나온 클래스를 최종 선택합니다.
* **Soft Voting** : 각 모델이 예측한 확률값을 평균 내어, 확률이 가장 높은 클래스를 최종 선택합니다. (일반적으로 성능이 더 좋아 많이 사용됩니다.)

---

### 6-3. 랜덤 포레스트(Random Forest)의 핵심

* **기본 철학** : 의사결정나무(Decision Tree)의 단점인 **이분법적 사고(과적합)** 와 **불안정성** 을 극복하기 위해, 수백 개의 나무를 모아 '숲'을 이루는 방식입니다.
* **작동 방식** :
1. **부트스트랩 샘플링(Bootstrap Sampling)** : 원본 데이터에서 중복을 허용하여 무작위로 여러 개의 샘플 데이터셋을 만듭니다.
2. **무작위 특징 선택** : 각 나무를 학습시킬 때 모든 변수를 쓰지 않고, 일부 변수만 무작위로 골라 학습시켜 나무들 간의 다양성을 확보합니다.
3. **집계** : 모든 나무가 내놓은 예측값의 평균(회귀)이나 다수결(분류)로 최종 답을 냅니다.



---

### 6-4. OOB(Out-of-Bag) 샘플이란?

* **정의** : 부트스트랩 샘플링 과정에서 **선택되지 않은 나머지 데이터(약 36.8%)** 를 말합니다.
* **활용** : 이 데이터는 학습에 사용되지 않았으므로, 별도의 테스트 세트 없이도 **모델의 성능을 평가(검증)** 하는 용도로 사용할 수 있습니다. 일종의 '자체 수능 시험지' 역할을 합니다.

---

### 6-5. 파이썬 실습: 변수 중요도(Feature Importance) 추출

랜덤 포레스트는 학습 후 어떤 변수가 예측(예: 해지 여부 `cancel_yn`)에 가장 큰 영향을 주었는지 알려주는 기능을 제공합니다.

```python
from sklearn.ensemble import RandomForestClassifier
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# 1. 모델 생성 및 학습
# n_estimators=100: 100개의 의사결정나무를 만듭니다.
rf_model = RandomForestClassifier(n_estimators=100, random_state=42)
rf_model.fit(X_train, y_train)

# 2. 변수 중요도(Feature Importance) 추출
importances = rf_model.feature_importances_

# 3. 중요도를 데이터프레임으로 변환 및 정렬
feature_import_df = pd.DataFrame({
    'Feature': X_train.columns,
    'Importance': importances
}).sort_values(by='Importance', ascending=False)

# 4. 시각화 (cancel_yn과 관련이 깊은 순서대로 출력)
plt.figure(figsize=(10, 6))
sns.barplot(x='Importance', y='Feature', data=feature_import_df)
plt.title('Feature Importances for Predicting Cancel_YN')
plt.show()

# [결과 해석] 
# 상단에 위치한 변수일수록 'cancel_yn(해지 여부)'을 결정짓는 핵심 지표임을 의미합니다.

```
